---
layout: post
title: Infrastructure - Hardware
subtitle: General IT systems and specialised broadcast gear. 
---

Welcome back! It's now time for us to explore some of the hardware infrastructure typically found within a post production studio, including both regular computing technology and specialised broadcast equipment. Once we've completed our investigation, we'll spend some time thinking about the implications for our home studio project. Let's get started!

# Overview 

One challenge commonly faced by smaller vfx and animation companies is that the scale of the computing infrastructure needed to support their day-to-day operations is often far greater than that which would normally be required to support an enterprise of a similar size within a different business sector. 

What this usually means is that the equipment that the business requires will generally be targetted towards a much larger enterprise, and is naturally priced accordingly. Thus, you need to be ready to negotiate with different solutions providers and apply some creative thinking in order to solve some of the infrastructure needs which arise as the business grows.

The positive side of this challenge is that (budget permitting) you'll have the opportunity to install and configure some awe-inspiring high-end computer systems and get involved in some really interesting infrastructure projects. Although this can be quite stressful at times, it can also be a lot of fun and a great way to learn new skills.

So, what are some of the chief attributes to take into consideration when evaluating hardware to purchase for your studio? Well, these would be my suggestions:

 - *Performance*: No matter how fast your equipment is, your artists will always push it to its limits.
 - *Scalability*: Systems need to be designed to scale capacity quickly, in order to meet production deadlines and support rapid business growth.
 - *Robustness*: Hardware platforms must be very reliable, as downtime can be costly to the business, and may have a serious impact on the ability of the studio to deliver a project.

The overall objective is to attempt to select equipment which will maximise the level of performance, scalability, and robustness, in order to provide the greatest possible overall value to the business for the given budget. 

# Components

In this section, we will review some key hardware infrastructure components which will be commonplace in any post-production studio.

## Compute

Artists will require high performance <a href="http://www8.hp.com/us/en/workstations/overview.html">workstations</a> equipped with powerful graphics cards, multiple (colour calibrated) displays, and (in some situations) specialised input devices such as graphics <a href="http://www.wacom.com/en-us/products">tablets</a> and pens. Production coordinators and supervisors tend to prefer laptop computers which they can bring with them to meetings and review sessions. 

Powerful rack-mount servers and blade systems are used to host back-end studio services and provide the necessary computing cycles for renders and automated pipeline operations. 

## Storage

The studio will require a scalable storage system that is able to maintain high performance and reliability under sustained load. These capabilities are normally provided by an enterprise class <a href="https://en.wikipedia.org/wiki/Network-attached_storage">NAS</a> cluster (e.g., <a href="https://en.wikipedia.org/wiki/EMC_Isilon">EMC Isilon</a>) or <a href="https://en.wikipedia.org/wiki/Storage_area_network">SAN</a> environment. 

Typically, such a system will comprise one or more arrays of spinning disks, high-bandwidth interconnects (such as <a href="https://en.wikipedia.org/wiki/InfiniBand">IB</a> or <a href="https://en.wikipedia.org/wiki/Fibre_Channel">FC</a>), a RAM or Flash based front-end caching layer, and a high capacity <a href="https://en.wikipedia.org/wiki/Linear_Tape-Open">LTO</a> tape backup library.

## Network

Each server and workstation requires a 1Gbps connection to the studio network, with some back-end systems (such as storage clusters and trunking links between core network switches) requiring much faster uplinks (e.g., 10Gbps and greater) in order to provide adequate bandwidth and alleviate network congestion. 

The core networking infrastructure is normally composed of one or more telecommunications-grade modular switches with fibre interconnects to each of the individual workgroup switches. Servers and render farm blades are often directly connected to high-density ethernet line cards installed in the core switches themselves.

The studio will also require a secure, high-bandwidth, connection to the outside world. If the facility is one of multiple sites, or utilising cloud resources, one or more <a href="https://en.wikipedia.org/wiki/Virtual_private_network">VPN</a> links may be necessary in order to carry traffic between the sites. Some studios subscribe to specialised networks (such as <a href="https://en.wikipedia.org/wiki/Sohonet">Sohonet</a>) which provide dedicated high-speed network connections between different post-production companies.

## Environment

Enterprise class server, network, and storage hardware can be very loud, power hungry, and generate a lot of heat, so a dedicated server room or on-site data centre with adequate cooling, noise suppression, and electricity supply to host the core infrastructure is essential. Fully populated server racks can also be very heavy (800+ kg or more), so the server room will need to be fitted out with apropriately load-rated flooring material.

Some critical systems may require access to a <a hef="https://en.wikipedia.org/wiki/Uninterruptible_power_supply">UPS</a>, to ensure that they are able to continue to operate (or shut down gracefully) in the event of a power outage. Environmental monitoring systems (e.g., temperature sensors) for the server room are also vital, as temperatures can quickly rise to dangerous levels should the cooling system fail.

## Broadcast Hardware 

Studios operating in the broadcast space will also often require access to some specialised hardware systems, such as tape decks for different analog video formats (for example, <a href="https://en.wikipedia.org/wiki/Betacam">DigiBeta</a>), video patching or switching systems, color accurate broadcast quality displays, and high end online <a href="http://www.avid.com/media-composer">editing</a>, <a href="https://www.filmlight.ltd.uk/products/baselight/overview_bl.php">grading</a>, or <a href="https://www.blackmagicdesign.com/products/davinciresolve">finishing</a> suites.

Much of this equipment is normally hosted within a dedicated tape room, with equipment racks and cooling systems similar to those used inside a data centre environment. Access to the broadcast equipment by human operators is quite frequent however, so it makes sense to keep it isolated from the core server and networking infrastructure for improved environmental and security control.

# The Cloud

Increasingly, post production studios are starting to move some of their hardware infrastructure into the cloud. My observation however, has been that take up of these services has been relatively slow compared to other industries, primarily due to security concerns and high-bandwidth network data transfer costs.

Probably the most frequent use case for cloud infrastructure in the vfx and animation industry that I am aware of is the utilisation of elastic compute resources to augment local render farm capacity during periods of peak demand (see <a href="https://aws.amazon.com/solutions/case-studies/fin-design-effects/">AWS Case Study: Fin Design + Effects</a>). This approach allows smaller studios to dynamically expand render capacity, which is especially useful in situations where on-site resources are over-subscribed and delivery deadlines are looming.

Dedicated cloud rendering services also exist (<a href="https://www.zyncrender.com/">ZYNC</a>, <a href="https://en.wikipedia.org/wiki/GreenButton">Green Button</a>), which can potentially eliminate the need for an on-site render farm entirely.

# Implications

What does all of this mean for the home vfx studio project? 

In a home environment, we'll obviously be operating on a much smaller scale, so our hardware needs will be more modest. We're also unlikely to have access to the kind of budget which is available to a commercial undertaking, so we'll want to limit our spending on hardware as much as possible, and try to make use of virtualisation and cloud based resources wherever we can.

One positive aspect of this approach (apart from the obvious cost savings!) is that most modern studios will be relying on a virtualisation platform to provide key services anyway, so by harnessing this technology, we'll be learning a useful skill that helps to achieve our goal to design and implement practical real-world solutions.

Later on, we might look into setting up some hardware systems for specific studio components, but to begin with (and especially while we're still in the development phase), we should be able to emulate the full home studio environment on a single development machine.

# Community

If you're currently employed in a technical role within the entertainment industry, you may wish to consider joining the <a href="http://studiosysadmins.com/">StudioSysAdmins</a> community. This helpful and friendly organisation provides resources and support for systems administrators and software development personnel working in games, vfx and animation studios around the world. 

# What's next?

In our next article, we will continue our exploration of studio infrastructure, and move on to some of the foundational software components and services that will act as the initial building blocks for higher-level vfx studio systems.

Once we're done, we'll be ready to set up a basic development environment, and start to take our first practical steps towards creating the infrastructure that we will need to get our home studio up and running. 
